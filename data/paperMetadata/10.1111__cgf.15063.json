{
  "paperId": "6487ec82f6d8082a5b402a5416ea03009acb1679",
  "externalIds": {
    "DBLP": "journals/cgf/PoYGABBCDHKLLMNOTWW24",
    "ArXiv": "2310.07204",
    "DOI": "10.1111/cgf.15063",
    "CorpusId": 263835355
  },
  "publicationVenue": null,
  "url": "https://www.semanticscholar.org/paper/6487ec82f6d8082a5b402a5416ea03009acb1679",
  "title": "State of the Art on Diffusion Models for Visual Computing",
  "abstract": "The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion\u2010based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state\u2010of\u2010the\u2010art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion\u2010based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.",
  "venue": "Computer graphics forum (Print)",
  "year": 2023,
  "referenceCount": 342,
  "openAccessPdf": null,
  "fieldsOfStudy": [
    "Computer Science"
  ],
  "s2FieldsOfStudy": [
    {
      "category": "Computer Science",
      "source": "external"
    },
    {
      "category": "Computer Science",
      "source": "s2-fos-model"
    }
  ],
  "publicationTypes": [
    "JournalArticle",
    "Review"
  ],
  "publicationDate": "2023-10-11",
  "journal": {
    "name": "Computer Graphics Forum",
    "volume": "43"
  },
  "citationStyles": {
    "bibtex": "@Article{Po2023StateOT,\n author = {Ryan Po and Wang Yifan and Vladislav Golyanik and Kfir Aberman and J. Barron and Amit H. Bermano and E. R. Chan and Tali Dekel and Aleksander Holynski and Angjoo Kanazawa and C. K. Liu and Lingjie Liu and B. Mildenhall and M. Nie\u00dfner and Bjorn Ommer and C. Theobalt and Peter Wonka and Gordon Wetzstein},\n booktitle = {Computer graphics forum (Print)},\n journal = {Computer Graphics Forum},\n title = {State of the Art on Diffusion Models for Visual Computing},\n volume = {43},\n year = {2023}\n}\n"
  },
  "authors": [
    {
      "authorId": "2142552068",
      "name": "Ryan Po"
    },
    {
      "authorId": "2140325536",
      "name": "Wang Yifan"
    },
    {
      "authorId": "3407706",
      "name": "Vladislav Golyanik"
    },
    {
      "authorId": "3451442",
      "name": "Kfir Aberman"
    },
    {
      "authorId": "50329510",
      "name": "J. Barron"
    },
    {
      "authorId": "2254256512",
      "name": "Amit H. Bermano"
    },
    {
      "authorId": "2257039304",
      "name": "E. R. Chan"
    },
    {
      "authorId": "2112779",
      "name": "Tali Dekel"
    },
    {
      "authorId": "2248172435",
      "name": "Aleksander Holynski"
    },
    {
      "authorId": "20615377",
      "name": "Angjoo Kanazawa"
    },
    {
      "authorId": "2257113317",
      "name": "C. K. Liu"
    },
    {
      "authorId": "46458089",
      "name": "Lingjie Liu"
    },
    {
      "authorId": "2577533",
      "name": "B. Mildenhall"
    },
    {
      "authorId": "2209612",
      "name": "M. Nie\u00dfner"
    },
    {
      "authorId": "2257038709",
      "name": "Bjorn Ommer"
    },
    {
      "authorId": "1680185",
      "name": "C. Theobalt"
    },
    {
      "authorId": "1798011",
      "name": "Peter Wonka"
    },
    {
      "authorId": "2256985147",
      "name": "Gordon Wetzstein"
    }
  ],
  "references": [
    {
      "paperId": "680131f23c287073f027435c29d9fc8b41902dee",
      "title": "Assessing User Perceptions of Bias in Generative AI Models: Promoting Social Awareness for Trustworthy AI"
    },
    {
      "paperId": "4584dee8505ce8cdaa09d7c3f4b4ab6568b3e766",
      "title": "RealFill: Reference-Driven Generation for Authentic Image Completion"
    },
    {
      "paperId": "d8d868912202cd7490a388dcbdeb0627f23bd435",
      "title": "Object Motion Guided Human Motion Synthesis"
    },
    {
      "paperId": "01ea770eeb43965985c9f7cfc6557b300a9f78b1",
      "title": "FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion"
    },
    {
      "paperId": "ef5d682a3efed36cdd3809d51a1a984f84c4b478",
      "title": "Generative Image Dynamics"
    },
    {
      "paperId": "de8412872a0577da2c082facca83af41ed60d599",
      "title": "TECA: Text-Guided Generation and Editing of Compositional 3D Avatars"
    },
    {
      "paperId": "aae4f9e196c695f398647d8780a982848df3f8ed",
      "title": "Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation"
    },
    {
      "paperId": "156fc6ed50e9873814dd8554526eedf9841a1be0",
      "title": "VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation"
    },
    {
      "paperId": "9aa01997226b5c4d705ae2e2f52c32681006654b",
      "title": "MVDream: Multi-view Diffusion for 3D Generation"
    },
    {
      "paperId": "5e2411967368081b3a1a083b115502b8fd58b1dc",
      "title": "HoloFusion: Towards Photo-realistic 3D Generative Modeling"
    },
    {
      "paperId": "8819777e104f8c4197c262e11a01b070b50007aa",
      "title": "MagicEdit: High-Fidelity and Temporally Coherent Video Editing"
    },
    {
      "paperId": "8aa2bddbea68bcdcf08f2f0ffb3ec829e27bddd8",
      "title": "Sparse3D: Distilling Multiview-Consistent Diffusion for Object Reconstruction from Sparse Views"
    },
    {
      "paperId": "fc96c4a7d4708bfd6138bfd16482229975404499",
      "title": "ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes"
    },
    {
      "paperId": "303f466fb823112f79a9f36637c7084dd8363fc5",
      "title": "TADA! Text to Animatable Digital Avatars"
    },
    {
      "paperId": "716c0bcf570943221d29be35226863a5337b07cb",
      "title": "Make-It-4D: Synthesizing a Consistent Long-Term Dynamic Scene Video from a Single Image"
    },
    {
      "paperId": "c2d65fc3a7fde3f7662c6ef9448e5737d7e5551f",
      "title": "CoDeF: Content Deformation Fields for Temporally Consistent Video Processing"
    },
    {
      "paperId": "84f0a99d0f0015a6145c94468870d43ab1d166fd",
      "title": "ModelScope Text-to-Video Technical Report"
    },
    {
      "paperId": "0904f2c00036193871ef6d8fc088fa9c780f6a7d",
      "title": "Generative AI meets Responsible AI: Practical Challenges and Opportunities"
    },
    {
      "paperId": "32d3048a4fe4becc7c4638afd05f2354b631cfca",
      "title": "SMPL: A Skinned Multi-Person Linear Model"
    },
    {
      "paperId": "ce5d807c8d8870a294fc901b8cbe401e3c11f4e3",
      "title": "TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis"
    },
    {
      "paperId": "2cc1d857e86d5152ba7fe6a8355c2a0150cc280a",
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering"
    },
    {
      "paperId": "4761f173965195798cd3046ef4af608a83504e4d",
      "title": "TokenFlow: Consistent Diffusion Features for Consistent Video Editing"
    },
    {
      "paperId": "06cba1df59ef16ef3e0cb641072fba43574d7c57",
      "title": "NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis"
    },
    {
      "paperId": "276f6117b8b8549a47461653b95e657278260ee3",
      "title": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models"
    },
    {
      "paperId": "1b90e9e9734bed6b379ae87d688cb3b887baf597",
      "title": "Objaverse-XL: A Universe of 10M+ 3D Objects"
    },
    {
      "paperId": "c1caa303549764d220ff17dc1785985dd1ba6047",
      "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning"
    },
    {
      "paperId": "5eda0509a04c85c7f69773a741a0310a5193d044",
      "title": "Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models"
    },
    {
      "paperId": "2cfaa5b3571d3b75f040f6d639359a3c673f5561",
      "title": "DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models"
    },
    {
      "paperId": "9f3a2c29413339c2ce01121ba701b02b239eef8c",
      "title": "Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset"
    },
    {
      "paperId": "830ed65d226a337088d6d97c3c3b068155bf10ec",
      "title": "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation"
    },
    {
      "paperId": "d212fa27f5868f0fd106e1a7bba908fd47da0816",
      "title": "MotionGPT: Human Motion as a Foreign Language"
    },
    {
      "paperId": "03a281a176413ed4d140293edc5bf04a3ad7a1f1",
      "title": "DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing"
    },
    {
      "paperId": "029f3e2c215edac138be26ade67b3d70b8f74dd7",
      "title": "DreamEditor: Text-Driven 3D Scene Editing with Neural Fields"
    },
    {
      "paperId": "b8972cd8e7f433ce297eef411256f54be4de3d02",
      "title": "EMoG: Synthesizing Emotive Co-speech 3D Gesture with Diffusion Model"
    },
    {
      "paperId": "489b35b66fab138ea28ade179f68135a1cd06ff9",
      "title": "Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision"
    },
    {
      "paperId": "1799398201d38f527cd0edcd23024b053984c4ee",
      "title": "DreamHuman: Animatable 3D Avatars from Text"
    },
    {
      "paperId": "de374dc9bb0b443ef399fc36587aa1e192447466",
      "title": "DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data"
    },
    {
      "paperId": "1e09b83fe064826a9a1ac61a7bdc00f26be41aee",
      "title": "Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation"
    },
    {
      "paperId": "a59373155cd7f27de4687bcb97c02e2ed5926a9b",
      "title": "Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data"
    },
    {
      "paperId": "a8e6a8480543efdfd5189a19c6c54a40d2cc6efe",
      "title": "BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping"
    },
    {
      "paperId": "16ff88634544e3a0b07246f59580495ac66a5bbf",
      "title": "ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections"
    },
    {
      "paperId": "f421b314aaff48e463507034691cfdd3f93cd4c2",
      "title": "Emergent Correspondence from Image Diffusion"
    },
    {
      "paperId": "f02ea7a18f00859d9ea1b321e3385ae7d0170639",
      "title": "VideoComposer: Compositional Video Synthesis with Motion Controllability"
    },
    {
      "paperId": "fbebb1a5d72aec2a6b13fc909f781f6ba9b04925",
      "title": "Diffusion Self-Guidance for Controllable Image Generation"
    },
    {
      "paperId": "861370f7c2d18bed09905fde334a19cc96e83e14",
      "title": "StyleDrop: Text-to-Image Generation in Any Style"
    },
    {
      "paperId": "2f319a4b6aa091073af5dfea4201ec19bda66b6c",
      "title": "AvatarStudio: Text-Driven Editing of 3D Dynamic Human Head Avatars"
    },
    {
      "paperId": "4f7430de11931b2f4e32807d14a51f701e2381ac",
      "title": "ShapeTalk: A Language Dataset and Framework for 3D Shape Edits and Deformations"
    },
    {
      "paperId": "9b504916f0e8fbeb1891f0db299dcbbc118f4898",
      "title": "SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds"
    },
    {
      "paperId": "5728ecb3a11c1586c4ae53e11ab395a0263eb5f4",
      "title": "Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models"
    },
    {
      "paperId": "ab4036bf29853d4b6e454184730eb6d26cbc4bc7",
      "title": "Break-A-Scene: Extracting Multiple Concepts from a Single Image"
    },
    {
      "paperId": "c5e9fd131cde68c218d0ea69cd617a67c7f35d42",
      "title": "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation"
    },
    {
      "paperId": "08a533c6f36f146adadd1edcc66162e88c93991b",
      "title": "NAP: Neural 3D Articulation Prior"
    },
    {
      "paperId": "c267f83a5d0fb0e4ed4c5c1174998ab6efd457aa",
      "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence"
    },
    {
      "paperId": "05b15934d837dc84afa96824742d3dcc7ec88e09",
      "title": "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold"
    },
    {
      "paperId": "1a3d6119d9513ad27fa4fc3262e517ec6a6d2261",
      "title": "FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention"
    },
    {
      "paperId": "50656eea23a65a716315ba88a0f741966d798d5e",
      "title": "AMD: Autoregressive Motion Diffusion"
    },
    {
      "paperId": "e01ed6611f9c998c237cda814ff8366a5acb6c3d",
      "title": "Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era"
    },
    {
      "paperId": "91ffe577a053ab0afccc7c031a903d8d40cec512",
      "title": "HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion"
    },
    {
      "paperId": "3985b81334ab90fbcbb30bbb3cc2840d00d509ba",
      "title": "DiffuseStyleGesture: Stylized Audio-Driven Co-Speech Gesture Generation with Diffusion Models"
    },
    {
      "paperId": "5f26421d5ae2c7721dedfa930cfe80b538d057a4",
      "title": "Locally Attentional SDF Diffusion for Controllable 3D Shape Generation"
    },
    {
      "paperId": "79d02a71b0b8a0f69bb340b10770544bca5d40d4",
      "title": "NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads"
    },
    {
      "paperId": "dc7b2a39421f93ae134fbb43ef62763056ebe1c7",
      "title": "Shap-E: Generating Conditional 3D Implicit Functions"
    },
    {
      "paperId": "642464e5edb3e45413cd6a410f8a067dd022c24f",
      "title": "LumiGAN: Unconditional Generation of Relightable 3D Human Faces"
    },
    {
      "paperId": "c5d53ec44aa0506cc0daea4483cedfb9ec82dd90",
      "title": "NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models"
    },
    {
      "paperId": "f5a0c57f90c6abe31482e9f320ccac5ee789b135",
      "title": "Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"
    },
    {
      "paperId": "6e1672fc5bdc874dbc9d9d1420b299a413173e0b",
      "title": "Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model"
    },
    {
      "paperId": "85963807c11abe38e9a2797d9860e012238607ef",
      "title": "MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing"
    },
    {
      "paperId": "398e91b99a4d988a39cadaa2a2c8ad9307615a5e",
      "title": "Delta Denoising Score"
    },
    {
      "paperId": "18e5fecd0ce09ac71706147393301dd25f42b359",
      "title": "Control3Diff: Learning Controllable 3D Diffusion Models from Single-view Images"
    },
    {
      "paperId": "78869b78b5c1e02fcfa39edd034bb7a1bdb24c4d",
      "title": "Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction"
    },
    {
      "paperId": "390eabbedf9a25d82eefd01a484b1f81fdd801df",
      "title": "InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions"
    },
    {
      "paperId": "34e95464be6cc3041041f145758493401b8a75e8",
      "title": "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"
    },
    {
      "paperId": "9733025aea2ba71792be10c18d635e8fc1455e31",
      "title": "InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning"
    },
    {
      "paperId": "cf923fb70bbad20c485cef355444a08096747f68",
      "title": "Generative Novel View Synthesis with 3D-Aware Diffusion Models"
    },
    {
      "paperId": "90e4ebaa16df6ead9c34e31d0621aae4e5739ce2",
      "title": "Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models"
    },
    {
      "paperId": "5ed27de050c7c31769af6e15ceae2dfcf0830c3f",
      "title": "Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion"
    },
    {
      "paperId": "0fa1501c7378a0dca2ac913fce9dcdcc2b1958a7",
      "title": "DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models"
    },
    {
      "paperId": "83b8e18488d8f31dd017ec0b26531cef4b635b36",
      "title": "Subject-driven Text-to-Image Generation via Apprenticeship Learning"
    },
    {
      "paperId": "73782be72f7188ef4a24482435352ec22d49e710",
      "title": "DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance"
    },
    {
      "paperId": "65edf9188b698091de0591d459fe212e6d00ea16",
      "title": "CIRCLE: Capture In Rich Contextual Environments"
    },
    {
      "paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
      "title": "A Survey of Large Language Models"
    },
    {
      "paperId": "836f0d803332853bb12a89495ea30f0e91c97bf6",
      "title": "AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control"
    },
    {
      "paperId": "0d5ad7985f6b4c590f2499cb361434debdce12b2",
      "title": "DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder"
    },
    {
      "paperId": "525e04db32139395dcf7125cf330dae35af84f35",
      "title": "4D Facial Expression Diffusion Model"
    },
    {
      "paperId": "5a2687fd4039fed76a6a139e4d5e8739dd72bd89",
      "title": "HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images"
    },
    {
      "paperId": "41e0e73cbacc7cf96f051dfbd0ce64ae1ad02b81",
      "title": "HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion"
    },
    {
      "paperId": "87c9bdef715883b63bd76ff0714ee998a3bc3188",
      "title": "GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents"
    },
    {
      "paperId": "923a03032014a12c4e8b26511c0394e1b915fe74",
      "title": "Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"
    },
    {
      "paperId": "32a3c2fbd3e733bd0eea938517fec2ff8dc7c701",
      "title": "Pix2Video: Video Editing using Image Diffusion"
    },
    {
      "paperId": "26c22380282a00166273038bc5ba785d845d61ad",
      "title": "Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions"
    },
    {
      "paperId": "fc64ff39e853791704f6b72eb824f4e86306fee1",
      "title": "Compositional 3D Scene Generation using Locally Conditioned Diffusion"
    },
    {
      "paperId": "602417aec279a68efeacfbf2df587384cbfef370",
      "title": "Vox-E: Text-guided Voxel Editing of 3D Objects"
    },
    {
      "paperId": "95aa6fa4e42387561cff22378348d528adea37f2",
      "title": "Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models"
    },
    {
      "paperId": "6ae34677bc41e1818a899583b25e379dadc42a85",
      "title": "SVDiff: Compact Parameter Space for Diffusion Fine-Tuning"
    },
    {
      "paperId": "2c70684973bc4d7b6f8404a647b8031c4d3c8383",
      "title": "Zero-1-to-3: Zero-shot One Image to 3D Object"
    },
    {
      "paperId": "14ccb8bcceb6de10eda6ad08bec242a4f2946497",
      "title": "FateZero: Fusing Attentions for Zero-shot Text-based Video Editing"
    },
    {
      "paperId": "1d70fb149cdee56c50843891dd17f9f4d5a72ff4",
      "title": "P+: Extended Textual Conditioning in Text-to-Image Generation"
    },
    {
      "paperId": "26c6090b7e7ba4513f82aa28d41360c60770c618",
      "title": "VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation"
    },
    {
      "paperId": "ffafc0b45a26aa5b3f6e235110e50572f0095929",
      "title": "3DGen: Triplane Latent Diffusion for Textured Mesh Generation"
    },
    {
      "paperId": "6283502d6900a0b403e2454b1cb1cf16ddefd5a7",
      "title": "Video-P2P: Video Editing with Cross-attention Control"
    },
    {
      "paperId": "817de38b0d2f27f1b7a1b5bdb0d1a52ac5fdcdf6",
      "title": "TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation"
    },
    {
      "paperId": "cb89947993021a3ee3c9f2bb926f6cab71d39793",
      "title": "Human Motion Diffusion as a Generative Prior"
    },
    {
      "paperId": "ac974291d7e3a152067382675524f3e3c2ded11b",
      "title": "Consistency Models"
    },
    {
      "paperId": "e15900cf7c93d4b6e45a12fe3534840c910467e1",
      "title": "ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation"
    },
    {
      "paperId": "35df2a7d78a8b7057d40d73bbb9a61f4643f92c4",
      "title": "Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models"
    },
    {
      "paperId": "a8efa7087fd6d84d5d84fd6c7c7cfb9d7ddb6dce",
      "title": "NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion"
    },
    {
      "paperId": "26e5b933b8f60bd749d428b5ff813b2abcd765d8",
      "title": "Composer: Creative and Controllable Image Synthesis with Composable Conditions"
    },
    {
      "paperId": "58842cdca3ea68f7b9e638b288fc247a6f26dafc",
      "title": "T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models"
    },
    {
      "paperId": "3c7582bf1a6682f6d33c47fa0c911e55823a3c44",
      "title": "Video Probabilistic Diffusion Models in Projected Latent Space"
    },
    {
      "paperId": "efbe97d20c4ffe356e8826c01dc550bacc405add",
      "title": "Adding Conditional Control to Text-to-Image Diffusion Models"
    },
    {
      "paperId": "07be0ec1f45e21a1032616535d0290ee6bfe0f6b",
      "title": "Structure and Content-Guided Video Synthesis with Diffusion Models"
    },
    {
      "paperId": "daf61010eee0fbf6f9bab7db71c395ffca6f3ff3",
      "title": "Zero-shot Image-to-Image Translation"
    },
    {
      "paperId": "45ce01446a0b85e84aa29c3209f25e0844a1cd45",
      "title": "SceneScape: Text-Driven Consistent Scene Generation"
    },
    {
      "paperId": "304cbe454a0239401f3d88fde55045f99fe90549",
      "title": "Shape-Aware Text-Driven Layered Video Editing"
    },
    {
      "paperId": "eb35863662544c977780299c21e669555ae83e81",
      "title": "3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models"
    },
    {
      "paperId": "d684fbe07585be651cc93d3c00ae3fe6df3ac877",
      "title": "Text-To-4D Dynamic Scene Generation"
    },
    {
      "paperId": "eb758dc90fadc306a71fb9f8ab1d3f691eac66fd",
      "title": "DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model"
    },
    {
      "paperId": "a9c0d7bad124697e370a1b345508b6bc618a4f64",
      "title": "Ethics of generative AI"
    },
    {
      "paperId": "a6c3dce03b16c366ba28dfa2232fb68d562e5474",
      "title": "HexPlane: A Fast Representation for Dynamic Scenes"
    },
    {
      "paperId": "8848d097f07bae98ff01094cf87e59eee94e4692",
      "title": "OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation"
    },
    {
      "paperId": "2b8ac06e792a64810414fa08fdaaac754df5573c",
      "title": "A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction"
    },
    {
      "paperId": "1367dcff4ccb927a5e95c452041288b3f0dd0eff",
      "title": "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"
    },
    {
      "paperId": "1b8a734dd28a9d766a5d3dbc0871e76b6a452b65",
      "title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts"
    },
    {
      "paperId": "1b31dbf44e68b698120552366df03e6e35a1e428",
      "title": "Objaverse: A Universe of Annotated 3D Objects"
    },
    {
      "paperId": "7e993a9ca01dcd4538362454aaac29a18a63c000",
      "title": "RODIN: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion"
    },
    {
      "paperId": "26a217cd87d4c054361fea6e74f8803a5a415293",
      "title": "ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding"
    },
    {
      "paperId": "7694f004c67840d7f098b3612d4b3dabd915c116",
      "title": "Executing your Commands via Motion Diffusion in Latent Space"
    },
    {
      "paperId": "02c0b857b13030a596bd34dc0d75f499aaf4b420",
      "title": "SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation"
    },
    {
      "paperId": "dbdfd1623586009305a3e4965bf2c233a46aea5a",
      "title": "MoFusion: A Framework for Denoising-Diffusion-Based Motion Synthesis"
    },
    {
      "paperId": "144eca44e250cc462f6fc3a172abb865978f66f5",
      "title": "Multi-Concept Customization of Text-to-Image Diffusion"
    },
    {
      "paperId": "67da7b3b32745fedf36a4906070049df18455fd7",
      "title": "Learning Neural Parametric Head Models"
    },
    {
      "paperId": "790f97e906dd361da16bee78d3c81c5c6bb36833",
      "title": "Diffusion-SDF: Text-to-Shape via Voxelized Diffusion"
    },
    {
      "paperId": "9d5e20f90fa3e7b126641ac6df5f42478ff7fb24",
      "title": "PhysDiff: Physics-Guided Human Motion Diffusion Model"
    },
    {
      "paperId": "c8e8c209054163abb258790ce37890329753546b",
      "title": "DiffRF: Rendering-Guided 3D Radiance Field Diffusion"
    },
    {
      "paperId": "fc011ed5ee986332523a62d2783adee1179dc1ed",
      "title": "Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation"
    },
    {
      "paperId": "b4ece600c6dadd41b0b38d8359ce8e5b544305a9",
      "title": "SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction"
    },
    {
      "paperId": "69d8fbd6721a490ca58116242274d642e3a9bbd9",
      "title": "3D Neural Field Generation Using Triplane Diffusion"
    },
    {
      "paperId": "55036dea7f6068d6b5de6ffe178bb324d01918a0",
      "title": "Sketch-Guided Text-to-Image Diffusion Models"
    },
    {
      "paperId": "b000d6865db824af1563708fb7a545ddd65c6b3a",
      "title": "Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation"
    },
    {
      "paperId": "7e80f79472d9b5aaa109075910d3ff9f9149f4c9",
      "title": "EDICT: Exact Diffusion Inversion via Coupled Transformations"
    },
    {
      "paperId": "732b007c75fae397da182f9a1ae132f2e2e30d05",
      "title": "MagicPony: Learning Articulated 3D Animals in the Wild"
    },
    {
      "paperId": "60ec9d5580bb3266c2cc8606887ac5663c44b461",
      "title": "Tensor4D: Efficient Neural 4D Decomposition for High-Fidelity Dynamic Reconstruction and Rendering"
    },
    {
      "paperId": "94b690162ead76af6a487d6e10998ea585c035d1",
      "title": "MagicVideo: Efficient Video Generation With Latent Diffusion Models"
    },
    {
      "paperId": "465b80aea2b99ffe0710325f5c256e8d2ac642a5",
      "title": "EDGE: Editable Dance Generation From Music"
    },
    {
      "paperId": "bdf4af8311637c681904e71cf50f96fd0026f578",
      "title": "Magic3D: High-Resolution Text-to-3D Content Creation"
    },
    {
      "paperId": "a2d2bbe4c542173662a444b33b76c66992697830",
      "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions"
    },
    {
      "paperId": "f352c11fad126a22deded3f904aa87eca5fc5df1",
      "title": "Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models"
    },
    {
      "paperId": "418df9e61ceb96f9f44a8afceb864a282462be14",
      "title": "RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation"
    },
    {
      "paperId": "793939b83e10903f58d8edbb7534963df627a1fe",
      "title": "Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures"
    },
    {
      "paperId": "258fff0da2ae4a7d6e94447c300bdaf7316f103b",
      "title": "CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language"
    },
    {
      "paperId": "23e261a20a315059b4de5492ed071c97a20c12e7",
      "title": "Imagic: Text-Based Real Image Editing with Diffusion Models"
    },
    {
      "paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
      "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"
    },
    {
      "paperId": "9793e6f3883e5ce5dbb92c0b940df06021ece1ae",
      "title": "LION: Latent Point Diffusion Models for 3D Shape Generation"
    },
    {
      "paperId": "36a5328c337697b96c9e6a9a04df0c924aa421f7",
      "title": "Unifying Diffusion Models' Latent Space, with Applications to CycleDiffusion and Guidance"
    },
    {
      "paperId": "c69361b20ad3d88ad4c5e9e1a3a66d0932f2bc43",
      "title": "GENIE: Higher-Order Denoising Diffusion Solvers"
    },
    {
      "paperId": "625d57bd52c60cd79aa4add6c4420dc2ad3b808a",
      "title": "On Distillation of Guided Diffusion Models"
    },
    {
      "paperId": "23a1b0fdb857fbbe328ffe254df33ff615acc5ea",
      "title": "Novel View Synthesis with Diffusion Models"
    },
    {
      "paperId": "498ac9b2e494601d20a3d0211c16acf2b7954a54",
      "title": "Imagen Video: High Definition Video Generation with Diffusion Models"
    },
    {
      "paperId": "15736f7c205d961c00378a938daffaacb5a0718d",
      "title": "Human Motion Diffusion Model"
    },
    {
      "paperId": "1e33716e8820b867d5a8aaebab44c2d3135ea4ac",
      "title": "Make-A-Video: Text-to-Video Generation without Text-Video Data"
    },
    {
      "paperId": "4c94d04afa4309ec2f06bdd0fe3781f91461b362",
      "title": "DreamFusion: Text-to-3D using 2D Diffusion"
    },
    {
      "paperId": "0ab5fa0d4b18d655e22ca7fec7c321ac7158dbca",
      "title": "Neural Wavelet-domain Diffusion for 3D Shape Generation"
    },
    {
      "paperId": "c17a983b11381fabb53f28066f76d4b2dc5a6a17",
      "title": "FLAME: Free-form Language-based Motion Synthesis & Editing"
    },
    {
      "paperId": "1bbf99b5bfe9869876ac3bdd2999e16b2632c283",
      "title": "MotionDiffuse: Text-Driven Human Motion Generation With Diffusion Model"
    },
    {
      "paperId": "5b19bf6c3f4b25cac96362c98b930cf4b37f6744",
      "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"
    },
    {
      "paperId": "04e541391e8dce14d099d00fb2c21dbbd8afe87f",
      "title": "Prompt-to-Prompt Image Editing with Cross Attention Control"
    },
    {
      "paperId": "5406129d9d7d00dc310671c43597101b0ee93629",
      "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"
    },
    {
      "paperId": "54e5843327857ed9041beb627dc0745db20c717a",
      "title": "GAUDI: A Neural Architect for Immersive 3D Scene Generation"
    },
    {
      "paperId": "af9f365ed86614c800f082bd8eb14be76072ad16",
      "title": "Classifier-Free Diffusion Guidance"
    },
    {
      "paperId": "df6987f29e53e5f95aec8fe520c16ac1428986b2",
      "title": "InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images"
    },
    {
      "paperId": "026c7b5f9cd232667d289ba2113cc21a0c18c178",
      "title": "Generalizable Patch-Based Neural Rendering"
    },
    {
      "paperId": "50578c1e4777d6ec61bbff2d79605ea4659e2e44",
      "title": "LASSIE: Learning Articulated Shapes from Sparse Image Ensemble via 3D Part Discovery"
    },
    {
      "paperId": "be4e6139b6df5a3c90b842fe332d2baef889fe10",
      "title": "SNeRF"
    },
    {
      "paperId": "c91eec8943a376bfb15db40c9cc5679ed8c801d5",
      "title": "Generative Neural Articulated Radiance Fields"
    },
    {
      "paperId": "60f029c0627aca8d28eb1b8560fa3f8eb6c0ef63",
      "title": "Music-to-Dance Generation with Multiple Conformer"
    },
    {
      "paperId": "9438577954f3b7696813ef58b9e0005d216cd400",
      "title": "Generating Long Videos of Dynamic Scenes"
    },
    {
      "paperId": "2f4c451922e227cbbd4f090b74298445bbd900d0",
      "title": "Elucidating the Design Space of Diffusion-Based Generative Models"
    },
    {
      "paperId": "4ccb30b632d847764591481bde34613b69530692",
      "title": "Generating Diverse and Natural 3D Human Motions from Text"
    },
    {
      "paperId": "707bd332d2c21dc5eb1f02a52d4a0506199aae76",
      "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers"
    },
    {
      "paperId": "9695824d7a01fad57ba9c01d7d76a519d78d65e7",
      "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
    },
    {
      "paperId": "3890d82362d07064687a4b5e9024fc4c92921998",
      "title": "MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation"
    },
    {
      "paperId": "35ddd30024c4390d8e8f363d1a1e8c0e74654643",
      "title": "ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation"
    },
    {
      "paperId": "39498d2d9b15a31a4d31fedd85b76d1ebc2b1218",
      "title": "GIMO: Gaze-Informed Human Motion Prediction in Context"
    },
    {
      "paperId": "831f9c2ccd2017a5391709b190ed258d3655797a",
      "title": "BEHAVE: Dataset and Method for Tracking Human Object Interactions"
    },
    {
      "paperId": "3303e078759075ac13a57e90eeb87a7d307005fd",
      "title": "Deformable Sprites for Unsupervised Video Decomposition"
    },
    {
      "paperId": "70193b15bed1c42558b7cbc7ff14f878361b4c74",
      "title": "SunStage: Portrait Reconstruction and Relighting Using the Sun as a Light Stage"
    },
    {
      "paperId": "3b2a675bb617ae1a920e8e29d535cdf27826e999",
      "title": "Video Diffusion Models"
    },
    {
      "paperId": "c0e8812789e96f5a7aa3ad940dba1c237aec822d",
      "title": "Text2LIVE: Text-Driven Layered Image and Video Editing"
    },
    {
      "paperId": "0e8c3f15c210909a361ba3378d6fe2822ae4f93e",
      "title": "AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation"
    },
    {
      "paperId": "d1ca82960da359427557189f95e1c4037ddab5fc",
      "title": "Dual Diffusion Implicit Bridges for Image-to-Image Translation"
    },
    {
      "paperId": "78194075df46c7653b17cb47ba52d25c44ea774f",
      "title": "Playable Environments: Video Manipulation in Space and Time"
    },
    {
      "paperId": "d97e0adbade91d76b10e8790205a71877a9be42b",
      "title": "StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2"
    },
    {
      "paperId": "fd52fea12a2140219575794bbe9c19cedc905f88",
      "title": "Multimodal Image Synthesis and Editing: The Generative AI Era"
    },
    {
      "paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models"
    },
    {
      "paperId": "7002ae048e4b8c9133a55428441e8066070995cb",
      "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"
    },
    {
      "paperId": "3007b30891714b5b0cc0a41eb06f8a194d74993a",
      "title": "Efficient Geometry-aware 3D Generative Adversarial Networks"
    },
    {
      "paperId": "7a29010548167cb9f1df5560ce13a054efbd0b5a",
      "title": "EgoBody: Human Body Shape and Motion of Interacting People from Head-Mounted Devices"
    },
    {
      "paperId": "846f181fd27287ff85f084597fdbf206d935c1a5",
      "title": "PartGlot: Learning Shape Part Segmentation from Language Reference Games"
    },
    {
      "paperId": "e91f73aaef155391b5b07e6612f5346dea888f64",
      "title": "Plenoxels: Radiance Fields without Neural Networks"
    },
    {
      "paperId": "88e8801e4daf404d3d40f1648ef29faeb8e6d58a",
      "title": "Blended Diffusion for Text-driven Editing of Natural Images"
    },
    {
      "paperId": "97bee918b08c244eb2e54d41e8ea6da00a3e5dbf",
      "title": "N\u00dcWA: Visual Synthesis Pre-training for Neural visUal World creAtion"
    },
    {
      "paperId": "e1a3e6856b6ac6af3600b5954392e5368603fd1b",
      "title": "Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions"
    },
    {
      "paperId": "19c68c1e0cf0d47b96bd448e0ade1c4add0601d6",
      "title": "ARKitScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data"
    },
    {
      "paperId": "37c9c4e7648f639c0b36f150fc6c6c90b3682f4a",
      "title": "Palette: Image-to-Image Diffusion Models"
    },
    {
      "paperId": "b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df",
      "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"
    },
    {
      "paperId": "28fa2c85f891d4d22589d7a63f2c3a62bcb7b136",
      "title": "ABO: Dataset and Benchmarks for Real-World 3D Object Understanding"
    },
    {
      "paperId": "8f8dedb511c0324d1cb7f9750560109ca9290b5f",
      "title": "DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation"
    },
    {
      "paperId": "738e3e0623054da29dc57fc6aee5e6711867c4e8",
      "title": "CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation"
    },
    {
      "paperId": "0af19549e4563312f658a237ba7f743d8eae8bc1",
      "title": "BuildingNet: Learning to Label 3D Buildings"
    },
    {
      "paperId": "7806ad7885d732040cb1fbf23857bba5b6779edd",
      "title": "Layered neural atlases for consistent video editing"
    },
    {
      "paperId": "77327efa13f9e1defba258091bab5d406d047a0b",
      "title": "Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction"
    },
    {
      "paperId": "05f907e437a14d0db9b7479662d0cd587cd54634",
      "title": "imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose"
    },
    {
      "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
      "title": "On the Opportunities and Risks of Foundation Models"
    },
    {
      "paperId": "f671a09e3e5922e6d38cb77dda8d76d5ceac2a27",
      "title": "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations"
    },
    {
      "paperId": "36c95e3ef362742a5c1844257e8b79d3251a781e",
      "title": "Language Grounding with 3D Objects"
    },
    {
      "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
      "title": "LoRA: Low-Rank Adaptation of Large Language Models"
    },
    {
      "paperId": "4fffa5245d3972077c83614c2a08a47cb578631e",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"
    },
    {
      "paperId": "29da3be81905d577dac9144c1c3cb5c6678f7027",
      "title": "Neural actor"
    },
    {
      "paperId": "fe9bc34b3e3b181de6caee3ff79e9c5bf1bbcf98",
      "title": "BABEL: Bodies, Action and Behavior with English Labels"
    },
    {
      "paperId": "5b95fe41fc1a0ec1c95502fb215eba49c9d40e3b",
      "title": "Copyright in generative deep learning"
    },
    {
      "paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794",
      "title": "Diffusion Models Beat GANs on Image Synthesis"
    },
    {
      "paperId": "e137c4fcdd7be545f1001d5f590538387493f7f1",
      "title": "HuMoR: 3D Human Motion Model for Robust Pose Estimation"
    },
    {
      "paperId": "b0d8650d08e6041c8851bfbba70097536fc4fbcb",
      "title": "LASR: Learning Articulated Shape Reconstruction from a Monocular Video"
    },
    {
      "paperId": "567218e84585558bf18a7d63c13f632680e124f1",
      "title": "4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface"
    },
    {
      "paperId": "b57969ee9a83054ffd00b9a62fc834ac53f95fb3",
      "title": "Real-time deep dynamic characters"
    },
    {
      "paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35",
      "title": "Emerging Properties in Self-Supervised Vision Transformers"
    },
    {
      "paperId": "8762a085024dbc3de27999b5a75b8b9f82cafca0",
      "title": "3D Shape Generation and Completion through Point-Voxel Diffusion"
    },
    {
      "paperId": "bac87bdb1cabc35fafb8176a234d332ebcc02864",
      "title": "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"
    },
    {
      "paperId": "4d0620ba57ba064876f05fe5cb2c3fe04062fa03",
      "title": "Human POSEitioning System (HPS): 3D Human Pose Estimation and Self-localization in Large Scenes from Body-Mounted Sensors"
    },
    {
      "paperId": "dd9803a2461123a94c58971af615f55734821913",
      "title": "VLGrammar: Grounded Grammar Induction of Vision and Language"
    },
    {
      "paperId": "bc519f58ae61afbf6318d6e4239d2d565c7ba467",
      "title": "Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models"
    },
    {
      "paperId": "c32fd8ea1b3f2df410410fb18d569dede102c53a",
      "title": "Diffusion Probabilistic Models for 3D Point Cloud Generation"
    },
    {
      "paperId": "98e565fa06f6c7bf7c46833b5106b26dc45130c4",
      "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning"
    },
    {
      "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
      "title": "Learning Transferable Visual Models From Natural Language Supervision"
    },
    {
      "paperId": "de18baa4964804cf471d85a5a090498242d2e79f",
      "title": "Improved Denoising Diffusion Probabilistic Models"
    },
    {
      "paperId": "b1afeacdae33129e84406ee2ed635fd81ae6efa7",
      "title": "Learning Speech-driven 3D Conversational Gestures from Video"
    },
    {
      "paperId": "021bd56b6c95d0196f3b8e818c948ea0702b0836",
      "title": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++"
    },
    {
      "paperId": "af8faec7c0b8f4b2a28d42a86e0e7d499016c560",
      "title": "Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans"
    },
    {
      "paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c",
      "title": "Taming Transformers for High-Resolution Image Synthesis"
    },
    {
      "paperId": "b44657c2e33b043374be13ff065744fd201a9a02",
      "title": "Infinite Nature: Perpetual View Generation of Natural Scenes from a Single Image"
    },
    {
      "paperId": "4365f51fc270c55005adb794002685078a6fca1d",
      "title": "pixelNeRF: Neural Radiance Fields from One or Few Images"
    },
    {
      "paperId": "8d17d62952f141fe5c4948eeafb8be5a8db9d054",
      "title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis"
    },
    {
      "paperId": "633e2fbfc0b21e959a244100937c5853afca4853",
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations"
    },
    {
      "paperId": "ffd87206430e4d7657a77d8eff484d335ac4056c",
      "title": "3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics"
    },
    {
      "paperId": "014576b866078524286802b1d0e18628520aa886",
      "title": "Denoising Diffusion Implicit Models"
    },
    {
      "paperId": "22c3badd79d4ee60892705b34c59807a6e828850",
      "title": "Adversarial score matching and improved sampling for image generation"
    },
    {
      "paperId": "bbaae3188aed31df77272c9f30d2b8faaa407939",
      "title": "Speech gesture generation from the trimodal context of text, audio, and speaker identity"
    },
    {
      "paperId": "0808f356856bf8db3f5f24645dadc8e169fd14d2",
      "title": "GRAB: A Dataset of Whole-Body Human Grasping of Objects"
    },
    {
      "paperId": "b7b4996858b91bc4db5dd5ce13215b5422591529",
      "title": "Action2Motion: Conditioned Generation of 3D Human Motions"
    },
    {
      "paperId": "c3adba29b9ff94a97d56a21003df333ccfc25e08",
      "title": "Character controllers using motion VAEs"
    },
    {
      "paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a",
      "title": "Denoising Diffusion Probabilistic Models"
    },
    {
      "paperId": "f09d0892d4547629301401547bd79e9be6984801",
      "title": "Videoforensicshq: Detecting High-Quality Manipulated Face Videos"
    },
    {
      "paperId": "67dea28495cab71703993d0d52ca4733b9a66077",
      "title": "Jukebox: A Generative Model for Music"
    },
    {
      "paperId": "92b4c8deecee703569b9e909dfb88aa70e691219",
      "title": "The Creation and Detection of Deepfakes"
    },
    {
      "paperId": "c2496cead9423897cd37be77ddd6cf2f5d03f1c8",
      "title": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments"
    },
    {
      "paperId": "57cf4fa6ab46f705e42671b23eace2495214b53c",
      "title": "DeepCap: Monocular Human Performance Capture Using Weak Supervision"
    },
    {
      "paperId": "5a6732513a1dc0bea059543f208a7556e3e31067",
      "title": "Convolutional Occupancy Networks"
    },
    {
      "paperId": "f94a09614d0b6f5c7ac6ab4b0f6df21480f0c7a5",
      "title": "Voxceleb: Large-scale speaker verification in the wild"
    },
    {
      "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
      "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "paperId": "fbc5486a1ffb9039dbb5046b84f0eb32e4ce8eea",
      "title": "A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications"
    },
    {
      "paperId": "5434f7c3b212a83d3f01949048ed1da114758cff",
      "title": "DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection"
    },
    {
      "paperId": "94cd2e0eec118f7421aa57d41be2f7dbcd085ef6",
      "title": "DeepDeform: Learning Non-Rigid RGB-D Reconstruction With Semi-Supervised Data"
    },
    {
      "paperId": "b97b96dde1b252ed52998c3fe585009983c987a9",
      "title": "The Social Photo: On Photography and Social Media, by Nathan Jurgenson"
    },
    {
      "paperId": "fdde9a63877db44c1b6a7f5cff5d47f01465e793",
      "title": "Point-Voxel CNN for Efficient 3D Deep Learning"
    },
    {
      "paperId": "60f511eef446120fe59563ee976f948d5e3f9064",
      "title": "The Replica Dataset: A Digital Replica of Indoor Spaces"
    },
    {
      "paperId": "7bbfde97cb870408c4c78c9ec1c47c962b268b8d",
      "title": "Ego-Pose Estimation and Forecasting As Real-Time PD Control"
    },
    {
      "paperId": "9018a815a8a43ef8e27a9af55abdf6a270d7bc28",
      "title": "Shapeglot: Learning Language for Shape Differentiation"
    },
    {
      "paperId": "4be4707aba8d622a0553aa159dc92ae7f9af9c5e",
      "title": "Expressive Body Capture: 3D Hands, Face, and Body From a Single Image"
    },
    {
      "paperId": "690c817ab5be8017ff713fa1028669debde205af",
      "title": "AMASS: Archive of Motion Capture As Surface Shapes"
    },
    {
      "paperId": "8ccb88958358ea59bdc9b76f660b01c8f631b2c0",
      "title": "HoloGAN: Unsupervised Learning of 3D Representations From Natural Images"
    },
    {
      "paperId": "a1a19aaddf57c0546357d890d9269092ba0afb26",
      "title": "Semantic Image Synthesis With Spatially-Adaptive Normalization"
    },
    {
      "paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c",
      "title": "Parameter-Efficient Transfer Learning for NLP"
    },
    {
      "paperId": "b4f8c1353aa2d88cacfaef1b3afba74dbf427d89",
      "title": "FaceForensics++: Learning to Detect Manipulated Facial Images"
    },
    {
      "paperId": "4d4972aed5c331cf4e13ce886f279ee4781febb6",
      "title": "Recent Advances in 3D Data Acquisition and Processing by Time-of-Flight Camera"
    },
    {
      "paperId": "b59233aab8364186603967bc12d88af48cc0992d",
      "title": "Towards Accurate Generative Models of Video: A New Metric & Challenges"
    },
    {
      "paperId": "44f263915d0dbc4028dafd052a35efc94527ddd2",
      "title": "Investigating the use of recurrent motion modelling for speech gesture generation"
    },
    {
      "paperId": "907e0f3300f2da386150fcdae3e0a7623db887de",
      "title": "Stereo magnification"
    },
    {
      "paperId": "28e1c5fa724db6c4d927799730d13229c4a74bac",
      "title": "Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling"
    },
    {
      "paperId": "ecc24760bfb423fc9bfd3b34162fea66dea1633c",
      "title": "Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings"
    },
    {
      "paperId": "1dbe6021ddb511b70cfc30373ed9cd699af06bdd",
      "title": "Learning Category-Specific Mesh Reconstruction from Image Collections"
    },
    {
      "paperId": "2ac159f3eda10e6522baf2874330a48d18439c73",
      "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition"
    },
    {
      "paperId": "ca235ce0decdb4f80024a429a20ae4437ceae09e",
      "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition"
    },
    {
      "paperId": "9723066a5587e6267d8abfd7feefd0637a5a211c",
      "title": "Demystifying MMD GANs"
    },
    {
      "paperId": "d488ceb4ea85d4d31a933a16b0abd0af042d5b98",
      "title": "HP-GAN: Probabilistic 3D Human Motion Prediction via GAN"
    },
    {
      "paperId": "ebf0615fc4d98cf1dbe527c79146ce1e50dce9af",
      "title": "CARLA: An Open Urban Driving Simulator"
    },
    {
      "paperId": "8337441971f941716a9e525a67f37088eb01fd13",
      "title": "Matterport3D: Learning from RGB-D Data in Indoor Environments"
    },
    {
      "paperId": "46882d15fb3f6449b0021ac5980676d858590f02",
      "title": "MonoPerfCap"
    },
    {
      "paperId": "231af7dc01a166cac3b5b01ca05778238f796e41",
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
    },
    {
      "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "title": "Attention is All you Need"
    },
    {
      "paperId": "8674494bd7a076286b905912d26d47f7501c4046",
      "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"
    },
    {
      "paperId": "b61a3f8b80bbd44f24544dc915f52fd30bbdf485",
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"
    },
    {
      "paperId": "e52e37cd91366f07df1f98e88f87010f494dd16e",
      "title": "ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"
    },
    {
      "paperId": "03eb382e04cca8cca743f7799070869954f1402a",
      "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"
    },
    {
      "paperId": "7de5044fa5bbd9bce905647a8e4a6a354863ca3c",
      "title": "Monocular 3D Human Pose Estimation in the Wild Using Improved CNN Supervision"
    },
    {
      "paperId": "8acbe90d5b852dadea7810345451a99608ee54c7",
      "title": "Image-to-Image Translation with Conditional Adversarial Networks"
    },
    {
      "paperId": "e738d166c72513feaede2e19b4e713fd410cfce8",
      "title": "The KIT Motion-Language Dataset"
    },
    {
      "paperId": "571b0750085ae3d939525e62af510ee2cee9d5ea",
      "title": "Improved Techniques for Training GANs"
    },
    {
      "paperId": "6c7f040a150abf21dbcefe1f22e0f98fa184f41a",
      "title": "Generative Adversarial Text to Image Synthesis"
    },
    {
      "paperId": "a473f545318325ba23b7a6b477485d29777ba873",
      "title": "ViZDoom: A Doom-based AI research platform for visual reinforcement learning"
    },
    {
      "paperId": "b8e2e9f3ba008e28257195ec69a00e07f260131d",
      "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"
    },
    {
      "paperId": "9b686d76914befea66377ec79c1f9258d70ea7e3",
      "title": "ShapeNet: An Information-Rich 3D Model Repository"
    },
    {
      "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
      "title": "Rethinking the Inception Architecture for Computer Vision"
    },
    {
      "paperId": "6364fdaa0a0eccd823a779fcdd489173f938e91a",
      "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
    },
    {
      "paperId": "bac377d3a051899dbe0d7249ed5d3d0b22d57310",
      "title": "Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments"
    },
    {
      "paperId": "71b7178df5d2b112d07e45038cb5637208659ff7",
      "title": "Microsoft COCO: Common Objects in Context"
    },
    {
      "paperId": "872bae24c109f7c30e052ac218b17a8b028d08a0",
      "title": "A Connection Between Score Matching and Denoising Autoencoders"
    },
    {
      "paperId": "d08cc366a4a0192a01e9a7495af1eb5d9f9e73ae",
      "title": "A 3-D Audio-Visual Corpus of Affective Communication"
    },
    {
      "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
      "title": "ImageNet: A large-scale hierarchical image database"
    },
    {
      "paperId": "31ca4e864074747ca459e9aad06ffe1d655faa39",
      "title": "Composer"
    },
    {
      "paperId": "c7a5128b45edb4db9105ec5167210b887617ddf2",
      "title": "Reverse-time diffusion equation models"
    },
    {
      "paperId": "0d66c6982fe5fee7089242ad2f27a686f1cc60ca",
      "title": "On a Formula Concerning Stochastic Differentials"
    },
    {
      "paperId": "b29155a6f6ba113188b28b1351fcc694f5e67eec",
      "title": "Stochastic Differential Equations in a Differentiable Manifold"
    },
    {
      "paperId": "90428f3a8caa5082f825ebf3138514ddf273dae3",
      "title": "Supplementary Materials for: NULL-text Inversion for Editing Real Images using Guided Diffusion Models"
    },
    {
      "paperId": "5749fa6343d757caa7ef6b2a25ca14c14f22cee0",
      "title": "Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D Diffusion-based Editor"
    },
    {
      "paperId": "48a7c60b023ebd2cf0587df1cc09f1309fe51d28",
      "title": "Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models"
    },
    {
      "paperId": "76deae79c597c9f32a4c4a292b3c5de4f9bad2ea",
      "title": "Designing an Encoder for Fast Personalization of Text-to-Image Models"
    },
    {
      "paperId": "daa474faf231ed6b67b41cfdcdaa145eac4828fa",
      "title": "Generative AI Art: Copyright Infringement and Fair Use"
    },
    {
      "paperId": "7cbc6c1ad543ee7b9dc8a3ba7497fd6f23720f8f",
      "title": "Generative AI: Overview, Economic Impact, and Applications in Asset Management"
    },
    {
      "paperId": "3ce0a1ae648d6727ca8b336c15a1378e1cfd0969",
      "title": "Plotting Behind the Scenes: Towards Learnable Game Engines"
    },
    {
      "paperId": "34652f60e182e712913cb95600ef18f86a07a7f2",
      "title": "How Generative Ai Turns Copyright Law on its Head"
    },
    {
      "paperId": null,
      "title": "OPENAI: DALL\u00b7E 2 -openai"
    },
    {
      "paperId": null,
      "title": "SKETCHFAB: Sketchfab -sketchfab"
    },
    {
      "paperId": null,
      "title": "Syncdreamer: Learning to generate multiview-consistent images from a single-view"
    },
    {
      "paperId": "968ea63f7f1c3c2e839e1a61b81259cd2dd962ba",
      "title": "DiffDreamer: Consistent Single-view Perpetual View Generation with Conditional Diffusion Models"
    },
    {
      "paperId": "735fcf085059f419112b76f7217e7f1407efcbb0",
      "title": "Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths"
    },
    {
      "paperId": "76636eec94a5405c4490fea2d028e474cbe0b386",
      "title": "Deep Generative Models on 3D Representations: A Survey"
    },
    {
      "paperId": "33f3f31f871070f19b0c3e967a24e322bfc178f2",
      "title": "Retrieval-Augmented Diffusion Models"
    },
    {
      "paperId": "16bca170e3f7bf3205156e0ef3829b0517781d2e",
      "title": "Neural Dense Non-Rigid Structure from Motion with Latent Space Constraints"
    },
    {
      "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "paperId": "d72de969746030e7939226e3e96196e3918bba58",
      "title": "Protecting World Leaders Against Deep Fakes"
    },
    {
      "paperId": "d47959e1f541d09f31dca8317b2f13c844379c4a",
      "title": "MeshCNN: a network with an edge"
    },
    {
      "paperId": null,
      "title": "Photoshape: Photorealistic materials for large-scale shape collections"
    },
    {
      "paperId": "cc5a1cf7ad9d644f21a5df799ffbcb8d1e24abe1",
      "title": "MonoPerfCap : Human Performance Capture from Monocular Video WEIPENGXU , AVISHEKCHATTERJEE , MICHAELZOLLH\u00d6FER , HELGERHODIN , DUSHYANTMEHTA"
    },
    {
      "paperId": null,
      "title": "A dataset of 101 human action classes from videos in the wild"
    },
    {
      "paperId": null,
      "title": "Twitter post"
    },
    {
      "paperId": null,
      "title": "Unpredictable black boxes are terrible interfaces"
    },
    {
      "paperId": null,
      "title": "Midjourney"
    },
    {
      "paperId": null,
      "title": "Deepfloyd if"
    },
    {
      "paperId": null,
      "title": "State of the Art object reconstruction from sparse views, 2023"
    },
    {
      "paperId": null,
      "title": "30 best midjourney prompts to get amazing results"
    },
    {
      "paperId": null,
      "title": "The carbon foot-print of gpt-4"
    },
    {
      "paperId": null,
      "title": "Diffusion models in practice"
    }
  ]
}